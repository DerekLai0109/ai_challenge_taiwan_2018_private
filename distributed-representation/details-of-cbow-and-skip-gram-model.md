# Details of CBOW and Skip-gram models

The vector representations of words, or word embeddings, learned by word2vec carry semantic meanings with applications in various NLP tasks.  
Word embedding belongs to the unsupervised learning algorithms.

The two word2vec models, CBOW model and skip-gram model, proposed by Mikolov \[1\], \[2\] outperform other models much more.

Efficiency improvement techniques, hierarchical softmax and negative sampling, are proposed in \[2\].

The detailed derivations are provided in \[3\].

An interacitve demo is available at \[4\].



\[1\]

T. Mikolov, K. Chen, G. Corrado and J. Dean, Efficient estimation of word representations in vector space,

arXiv:1301.3781, 2013.

\[2\]

T. Mikolov, I. Sutskever, K. Chen, G. Corrado and J. Dean, Distributed representations of words and phrases and their compositionality,  arXiv:1301.4546, 2013.

\[3\]

X. Rong, word2vec parameter learning explained, arXiv:1411.2738, 2014.

\[4\]

http://bit.ly/wevi-online



